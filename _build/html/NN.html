
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Introduction &#8212; AI-Powered Data Exploration</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'NN';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Notebooks with MyST Markdown" href="markdown-notebooks.html" />
    <link rel="prev" title="Markdown Files" href="markdown.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="AI-Powered Data Exploration - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="AI-Powered Data Exploration - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="markdown.html">Markdown Files</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Introduction</a></li>



<li class="toctree-l1"><a class="reference internal" href="markdown-notebooks.html">Notebooks with MyST Markdown</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FNN.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/NN.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Introduction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#modeling-with-neural-networks">Modeling with Neural Networks</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning">Deep Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-architectures-for-specific-learning-challenges">Custom Architectures for Specific Learning Challenges</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computer-vision-convolutional-neural-networks">Computer Vision - Convolutional Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-data-recurrent-neural-networks">Sequential Data - Recurrent Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-data-transformer-neural-networks">Text Data - Transformer Neural Networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-weights-backpropogation-and-gradient-descent">Training the Weights: Backpropogation and Gradient Descent</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h1>
<p>In recent years there has been an explosion in the amount of available data, due to the extensive availability of low-cost computer memory and automated data collection systems. The regression modeling techniques of the past were developed to work well under ideal theoretical conditions provided basic assumptions are met. In practice however, they typically don’t work well with large data sets (observations in the thousands and feature space 50+ dimensions). In the modern era, it is not uncommon to be faced with data sets involving millions of observations and hundreds or thousands of predictors. This exponential growth in available data has motivated researchers in the’ fields of statistics, artificial intelligence, and data mining to develop a few brand new procedures for data modeling that can be applied to very large data sets. However, it turns out that flexible, powerful state of the art methods were discovered from theoretical standpoint decades (and even centuries, in the case of Bayesian updating) ago. the missing element has always been the ability of computers to efficiently execute these algorithms in user-friendly ways. The one technique that most exemplifies this narrative is the neural network.</p>
<p>Neural networks are patterned after the system of neurons and synapses in the human brain. Neurons define the process our brains use to react to stimuli around us. The following paragraph does a good job of explaining the process:</p>
<p><em>Neurons are typically classified into three types based on their function. <a class="reference external" href="https://en.wikipedia.org/wiki/Sensory_neuron" title="Sensory neuron">Sensory neurons</a> respond to <a class="reference external" href="https://en.wikipedia.org/wiki/Stimulus_(physiology)" title="Stimulus (physiology)">stimuli</a> such as touch, sound, or light that affect the cells of the <a class="reference external" href="https://en.wikipedia.org/wiki/Sense" title="Sense">sensory organs</a>, and they send signals to the spinal cord or brain. <a class="reference external" href="https://en.wikipedia.org/wiki/Motor_neuron" title="Motor neuron">Motor neurons</a> receive signals from the brain and spinal cord to control everything from <a class="reference external" href="https://en.wikipedia.org/wiki/Muscle_contraction" title="Muscle contraction">muscle contractions</a> to <a class="reference external" href="https://en.wikipedia.org/wiki/Gland" title="Gland">glandular output</a>. <a class="reference external" href="https://en.wikipedia.org/wiki/Interneuron" title="Interneuron">Interneurons</a> connect neurons to other neurons within the same region of the brain or spinal cord. When multiple neurons are functionally connected together, they form what is called a <a class="reference external" href="https://en.wikipedia.org/wiki/Neural_circuit" title="Neural circuit">neural circuit</a></em> (retrieved from <a class="reference external" href="https://en.wikipedia.org/wiki/Neuron">https://en.wikipedia.org/wiki/Neuron</a>).</p>
<p><img alt="" src="images/paste-D3518732.png" /></p>
<p>Image from:<a class="reference external" href="https://www.biolegend.com/de-at/synaptic-function">https://www.biolegend.com/de-at/synaptic-function</a></p>
<p>It is important to note that these “functional connections” result in unique architectures. For example the visual information received through our eyes is processed differently from the audio information processed through our ears.</p>
</section>
<section id="modeling-with-neural-networks">
<h1>Modeling with Neural Networks<a class="headerlink" href="#modeling-with-neural-networks" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
<p>The basic idea behind the neural network approach in regression and classification problems is to model the response as a <em>nonlinear</em> function of various [linear combinations]{.underline} of the predictors (Independent variables/inputs). One common aspect of every neural network is that it consists of an <strong>input layer</strong>, whose dimension is based on the nature of the features (predictors, independent, explanatory variables) in our data. Similarly, the dimension of the <strong>output</strong> layer is based on the target variable (response, label) we seek to predict.</p>
<p><img alt="" src="images/paste-37781E17.png" />{width=”478”}</p>
<p>Diagram Created with <a class="reference external" href="https://alexlenail.me/NN-SVG/index.html%20https://alexlenail.me/NN-SVG/index.html">https://alexlenail.me/NN-SVG/index.html</a></p>
<p>The image depicts a <em>single layer perceptron</em> neural network architecture. It is called a [feed forward]{.underline} network because data enters the network from the left and moves through the network until it exits on the right with a predicted value for our response. It is the most fundamental type of neural network we can work with. Let’s discuss the components:</p>
<p>The Input Layer consists of 5 neurons. 4 corresponding to the features in our data, and one corresponding to the intercept, or <em>bias</em> term we are choosing to include in our model.</p>
<p>The synapses connecting the input layer to the output layer are known as weights. These are unknown parameters that must be learned through our data. [If input variables are on an equal scale]{.underline}, then the values if the weights relative to each other give an indication as to which are more influential in predicting the outcome variable. Keeping variables on the same scale corrects over/under emphasizing variables simply based on order of magnitude. There are a number of pre-processing steps that can be used to put variables on a common scale. This step is common to many machine learning models, not just neural networks.</p>
<p>Standardization</p>
<p><span class="math notranslate nohighlight">\(X^*=\frac{X_{ij}-mean(X_j)}{stdev(X_j)}\)</span></p>
<p>MAX-MIN Normalization</p>
<p><span class="math notranslate nohighlight">\(X^*=\frac{X_{ij}-min(X_j)}{max(X_j)-min(X_j)}\)</span></p>
<p>This is the method of choice for neural networks, as it typicaly makes for a higher likelihood of model stability stability when the weights are being learned.</p>
<p>The neurons of the first layer form a linear combination through the weights to create the constant <span class="math notranslate nohighlight">\(Z_i=\beta_{0} + \beta_{1}X_{i1} +...+ \beta_{4}X_{i4}\)</span> for individual <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>The <em>activation function</em> <span class="math notranslate nohighlight">\(g\)</span> may be interpreted based on whether a regression or classification problem is being done. In the regression context, we can simply see <span class="math notranslate nohighlight">\(g\)</span> as a function used to model nonlinearity in the relationship between predictors and response. In the classification context, consider the biological context. When would an audio stimulus cause you to cover your ears? Analogously, in the model we could consider the activation potential being met if the value of <span class="math notranslate nohighlight">\(Z_i\)</span> exceeds 0, leading to a positive classification, and not otherwise. In other words, our classification rule is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{Y}_i=
    \begin{cases}        1 &amp; \text{if } Z_i \geq 0\\        
                         0 &amp; \text{if } Z_i &lt; 0    
    \end{cases}
\end{split}\]</div>
<p>The drawback to this rule is that it leads to a non-smooth function as the output of our network. We will soon find out that a critical part of training our model will be determining the location in the parameter space that minimizes some loss criterion. This means we’ll be taking derivatives, and possibly logarithms, so smoothness is a must. Thus we prefer activation functions with the same range from 0 to 1, but also continuously hitting all values in between. Popular choices are seen in the picture below.</p>
<p><img alt="" src="paste-1830B82C.png" /></p>
<p>How should one choose the “best” activation function? Unlike classical statistical methods where model assumptions dictate what is appropriate, in machine learning oftentimes it is the nature of the data, and performance that determine which action to take. In March 2023, researchers at MIT discovered a new type of activation function that seems to <a class="reference external" href="https://news.mit.edu/2023/method-designing-neural-networks-optimally-suited-certain-tasks-0330">outperform the standard options</a>. According to lead author, Adityanarayanan Radhakrishnan, “It turns out that, if you take the standard activation functions that people use in practice, and keep increasing the depth of the network, it gives you really terrible performance. We show that if you design with different activation functions, as you get more data, your network will get better and better.” The article was published in the <a class="reference external" href="https://www.pnas.org/doi/10.1073/pnas.2208779120">Proceedings of the Natural Academy of Sciences</a>.</p>
<p>It is a methodical interplay between the quantities defined above and the data to learn the values of the weights that are in some sense optimal.</p>
<p><strong>Exercise 1: The standard multiple regression model involves just one linear combination of the predictors, namely:</strong></p>
<div class="math notranslate nohighlight">
\[ \mu_i=E\{Y_{i}\} = \beta_{0} + \beta_{1}X_{i1} +...+ \beta_{p-1}X_{i,p-1} \]</div>
<p><strong>How would a neural network that mirrors this scenario look? Express directly and in vector/matrix notation. For a given data set, do you think the weights would be the same as the parameter estimates obtained via least squares?</strong></p>
<p><strong>Exercise 2: The standard logistic regression model involves just one linear combination of the predictors, namely:</strong></p>
<div class="math notranslate nohighlight">
\[ \pi_i=E\{Y_{i}\} = \frac{1}{1+\exp\left[-(\beta_{0} + \beta_{1}X_{i1} +...+ \beta_{p-1}X_{i,p-1})\right]} \]</div>
<p><strong>How would a neural network that mirrors this scenario look? Express directly and in vector/matrix notation. For a given data set, do you think the weights would be the same as the parameter estimates obtained via maximum likelihood?</strong></p>
<p>Despite the similarities, the neural network model (in particular deep learning models - where there are additional layers between the input and output) is simply a nonlinear statistical model that contains many more parameters than the corresponding linear statistical model. One result of this is that the models will typically be overparameterized, resulting in parameters that are uninterpretable, which is a major shortcoming of neural network modeling. An advantage of the neural network approach is that the resulting model will often perform better in predicting future responses than a standard regression model.</p>
</section>
<section id="deep-learning">
<h1>Deep Learning<a class="headerlink" href="#deep-learning" title="Link to this heading">#</a></h1>
<p>The single layer perceptron is powerful in itself as a generalization of familiar statistical models. But that doesn’t begin to scratch the surface on the massive array of customized architectures of arbitrary complexity that can be created in the neural net framework. It is important to note that model complexity and flexibility are not merely theoretical novelties. The theoretical properties of the neural net have allowed it to be applied to a wide variety of classical and modern problems in machine learning and artificial intelligence, with success metrics that often rival the human brains they were modeled after. All this while using high-powered computing. This being said, <strong>it is important to recognize that the nature of the input and output layers discussed above remain the same</strong>. The input layer will be dictated by the feature space, and the output layer by the problem we are tackling: regression, classification, etc. We must apply familiar principles of model training, testing and hyperparameter tuning to improve architecture towards optimality.</p>
<p><img alt="" src="images/paste-0D341572.png" />{width=”494”}</p>
<p>Example: General idea of Deep Learning</p>
<p><img alt="" src="images/paste-2CC859C1.png" />Example: A deep NN with (4,6,5,7,3) Architecture</p>
<p>The good news is that for a wide array of common tasks, this work has been done repeatedly with simulated and real data sets, and with computing power orders of magnitude greater than what we can do with consumer machines. The concept of <em>Transfer Learning</em> is the study of how to effectively apply pre-trained models to harness their predictive power without needing to start from scratch. This topic will be discussed in further detail later this semester.</p>
<p><strong>Example: Building a three-layer network for regression modelling.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the model</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Test the model</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([353])) that is different to the input size (torch.Size([353, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0 loss: 29704.7890625
Epoch 100 loss: 29549.458984375
Epoch 200 loss: 29315.44140625
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 300 loss: 28964.796875
Epoch 400 loss: 28355.515625
Epoch 500 loss: 27450.615234375
Epoch 600 loss: 26344.712890625
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 700 loss: 25104.158203125
Epoch 800 loss: 23778.962890625
Epoch 900 loss: 22400.25390625
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test loss: 17794.775390625
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([89])) that is different to the input size (torch.Size([89, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a new input to make predictions on</span>
<span class="n">new_input</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.02731</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">7.07</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.469</span><span class="p">,</span> <span class="mf">6.421</span><span class="p">,</span> <span class="mf">78.9</span><span class="p">,</span> <span class="mf">4.9671</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">242.0</span><span class="p">]]</span>

<span class="c1"># Standardize the input using the same scaler we used on the training data</span>
<span class="n">new_input</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_input</span><span class="p">)</span>

<span class="c1"># Convert the input to a PyTorch tensor</span>
<span class="n">new_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">new_input</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Make a prediction using the trained model</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">new_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction: </span><span class="si">{</span><span class="n">prediction</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prediction: 22062.53515625
</pre></div>
</div>
</div>
</div>
<section id="custom-architectures-for-specific-learning-challenges">
<h2>Custom Architectures for Specific Learning Challenges<a class="headerlink" href="#custom-architectures-for-specific-learning-challenges" title="Link to this heading">#</a></h2>
<section id="computer-vision-convolutional-neural-networks">
<h3>Computer Vision - Convolutional Neural Networks<a class="headerlink" href="#computer-vision-convolutional-neural-networks" title="Link to this heading">#</a></h3>
<p>Convolutional Neural Networks (CNNs) are a specific type of neural network that are particularly well-suited for processing image and video data. They are based on the concept of convolution, which involves sliding a filter over the input image and computing a dot product between the filter weights and the corresponding pixel values at each location. The formula for the convolution operation is:</p>
<p><span class="math notranslate nohighlight">\((f * w)(i, j) = ΣΣ f(m, n) * w(i-m, j-n)\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p>f is the input image</p></li>
<li><p>w is the kernel</p></li>
<li><p>i and j are the indices of the output feature map</p></li>
<li><p>m and n are the indices of the kernel</p></li>
</ul>
<p><img alt="" src="https://lh4.googleusercontent.com/8h8GN8pIRQAYb0pmxeMQAr-BK9g-RwSYDg5s7xPGf5MlafAPpQDTA4esBy11Ze-wevSgVxlRk9ud9IPfwFmUgojxEvowwFwc8YuX5vZrJsTG_Qb0_Dg2017hP3WWDRogqiUBbEdW7w4d" /></p>
<p>CNNs are typically composed of multiple layers, including convolutional layers, pooling layers, and fully connected layers. In a convolutional layer, the network “learns” to extract features from the input image by convolving it with a set of learned filters. The resulting feature maps capture different aspects of the image, such as edges, corners, and textures. Pooling layers are used to downsample the feature maps, reducing their dimensionality and making them more computationally efficient. This is typically done by taking the maximum or average value over a local region of the feature map. The formula for max pooling is:</p>
<p><span class="math notranslate nohighlight">\(y(i,j) = max { f(i',j') : i' ∈ [istride,istride+p), j' ∈ [jstride,jstride+p) }\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p>f is the input feature map</p></li>
<li><p>y is the output pooled feature map</p></li>
<li><p>i and j are the indices of the output feature map</p></li>
<li><p>p is the pooling window size</p></li>
<li><p>stride is the stride of the pooling operation</p></li>
</ul>
<p>Finally, fully connected layers are used to combine the extracted features and produce the final output. These layers are similar to those used in traditional neural networks, but they operate on the flattened feature maps rather than the raw input data.</p>
<p>During training, the weights of the CNN are adjusted using backpropagation and stochastic gradient descent, similar to traditional neural networks. However, because of the specific structure of CNNs, they can learn to extract high-level features from images and capture complex relationships between them.</p>
</section>
<section id="sequential-data-recurrent-neural-networks">
<h3>Sequential Data - Recurrent Neural Networks<a class="headerlink" href="#sequential-data-recurrent-neural-networks" title="Link to this heading">#</a></h3>
</section>
<section id="text-data-transformer-neural-networks">
<h3>Text Data - Transformer Neural Networks<a class="headerlink" href="#text-data-transformer-neural-networks" title="Link to this heading">#</a></h3>
</section>
</section>
</section>
<section id="training-the-weights-backpropogation-and-gradient-descent">
<h1>Training the Weights: Backpropogation and Gradient Descent<a class="headerlink" href="#training-the-weights-backpropogation-and-gradient-descent" title="Link to this heading">#</a></h1>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="markdown.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Markdown Files</p>
      </div>
    </a>
    <a class="right-next"
       href="markdown-notebooks.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Notebooks with MyST Markdown</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Introduction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#modeling-with-neural-networks">Modeling with Neural Networks</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning">Deep Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-architectures-for-specific-learning-challenges">Custom Architectures for Specific Learning Challenges</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computer-vision-convolutional-neural-networks">Computer Vision - Convolutional Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-data-recurrent-neural-networks">Sequential Data - Recurrent Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-data-transformer-neural-networks">Text Data - Transformer Neural Networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-weights-backpropogation-and-gradient-descent">Training the Weights: Backpropogation and Gradient Descent</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>